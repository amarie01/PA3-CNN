{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_cnn import *\n",
    "from baseline_cnn import BasicCNN\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return np.sum(predictions == labels) / float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(predictions, labels):\n",
    "    TP = np.sum(np.logical_and(predictions == labels, labels == 1))\n",
    "    FP = np.sum(np.logical_and(predictions != labels, predictions == 1))\n",
    "    return TP / float(FP + TP + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(predictions, labels):\n",
    "    TP = np.sum(np.logical_and(predictions == labels, labels == 1))\n",
    "    FN = np.sum(np.logical_and(predictions != labels, predictions == 0))\n",
    "    return TP / float(FN + TP + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCR(predictions, labels):\n",
    "    return (precision(predictions, labels) + recall(predictions, labels)) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(predictions, labels):\n",
    "    return np.sum(predictions == labels, axis=0) / float(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_per_class(predictions, labels):\n",
    "    TP = np.sum(np.logical_and(predictions == labels, labels == 1), axis=0)\n",
    "    FP = np.sum(np.logical_and(predictions != labels, predictions == 1), axis=0)\n",
    "    return TP / np.asfarray(FP + TP + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_per_class(predictions, labels):\n",
    "    TP = np.sum(np.logical_and(predictions == labels, labels == 1), axis=0)\n",
    "    FN = np.sum(np.logical_and(predictions != labels, predictions == 0), axis=0)\n",
    "    return TP / np.asfarray(FN + TP + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCR_per_class(predictions, labels):\n",
    "    return (precision_per_class(predictions, labels) + recall_per_class(predictions, labels)) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(mtx, predictions, actuals): \n",
    "    for p,a in zip(predictions, actuals):\n",
    "        \n",
    "        for i in range(p.shape[0]):\n",
    "            # If TP, add 1 to diagonal\n",
    "            # Then discard the other outputs\n",
    "            if p[i] == 1 and a[i] == 1:\n",
    "                mtx[i][i] += 1\n",
    "                \n",
    "            elif p[i] == 1:\n",
    "                mtx[i] += a\n",
    "    \n",
    "    return mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(batch_start, batch_count, accuracies, precisions, recalls, BCRs, aggregate=True):\n",
    "    if aggregate:\n",
    "        acc = np.mean(accuracies[batch_start:])\n",
    "        pre = np.mean(precisions[batch_start:])\n",
    "        rec = np.mean(recalls[batch_start:])\n",
    "        bcr = np.mean(BCRs[batch_start:])\n",
    "    else:\n",
    "        acc = np.mean(accuracies[batch_start:], axis=0)\n",
    "        pre = np.mean(precisions[batch_start:], axis=0)\n",
    "        rec = np.mean(recalls[batch_start:], axis=0)\n",
    "        bcr = np.mean(BCRs[batch_start:], axis=0)\n",
    "            \n",
    "    # Print the loss averaged over the last N mini-batches    \n",
    "    print('Minibatch ' + str(batch_count) + ' accuracy: ' + str(acc))\n",
    "    print('Minibatch ' + str(batch_count) + ' precision: ' + str(pre))\n",
    "    print('Minibatch ' + str(batch_count) + ' recall: ' + str(rec))\n",
    "    print('Minibatch ' + str(batch_count) + ' bcr: ' + str(bcr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 1           # Number of full passes through the dataset\n",
    "batch_size = 16          # Number of samples in each minibatch\n",
    "learning_rate = 0.0001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "# Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "transform = transforms.Compose([transforms.Resize(512), transforms.ToTensor()])\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "# Instantiate a BasicCNN to run on the GPU or CPU based on CUDA support\n",
    "model = BasicCNN()\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "# Define the loss criterion and instantiate the gradient descent optimizer\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average minibatch 49 loss: 0.450\n",
      "0.9710661910424099 % done,  56.34810996055603  Seconds elapsed\n",
      "Epoch 1, average minibatch 99 loss: 0.223\n",
      "1.9619500594530321 % done,  111.43220615386963  Seconds elapsed\n",
      "Epoch 1, average minibatch 149 loss: 0.221\n",
      "2.9528339278636544 % done,  164.38472437858582  Seconds elapsed\n",
      "Epoch 1, average minibatch 199 loss: 0.210\n",
      "3.9437177962742767 % done,  216.33850073814392  Seconds elapsed\n",
      "Epoch 1, average minibatch 249 loss: 0.212\n",
      "4.934601664684899 % done,  269.0460262298584  Seconds elapsed\n",
      "Epoch 1, average minibatch 299 loss: 0.204\n",
      "5.925485533095522 % done,  320.4929976463318  Seconds elapsed\n",
      "Epoch 1, average minibatch 349 loss: 0.195\n",
      "6.916369401506143 % done,  371.29173612594604  Seconds elapsed\n",
      "Epoch 1, average minibatch 399 loss: 0.200\n",
      "7.907253269916766 % done,  422.2179160118103  Seconds elapsed\n",
      "Epoch 1, average minibatch 449 loss: 0.204\n",
      "8.898137138327389 % done,  473.09689712524414  Seconds elapsed\n",
      "Epoch 1, average minibatch 499 loss: 0.197\n",
      "9.88902100673801 % done,  523.5078542232513  Seconds elapsed\n",
      "Epoch 1, average minibatch 549 loss: 0.199\n",
      "10.879904875148632 % done,  574.6238250732422  Seconds elapsed\n",
      "Epoch 1, average minibatch 599 loss: 0.197\n",
      "11.870788743559254 % done,  625.0833156108856  Seconds elapsed\n",
      "Epoch 1, average minibatch 649 loss: 0.199\n",
      "12.861672611969878 % done,  674.9771237373352  Seconds elapsed\n",
      "Epoch 1, average minibatch 699 loss: 0.197\n",
      "13.8525564803805 % done,  724.7174797058105  Seconds elapsed\n",
      "Epoch 1, average minibatch 749 loss: 0.191\n",
      "14.843440348791122 % done,  775.0273237228394  Seconds elapsed\n",
      "Epoch 1, average minibatch 799 loss: 0.192\n",
      "15.834324217201743 % done,  824.1834180355072  Seconds elapsed\n",
      "Epoch 1, average minibatch 849 loss: 0.181\n",
      "16.825208085612367 % done,  874.1424310207367  Seconds elapsed\n",
      "Epoch 1, average minibatch 899 loss: 0.188\n",
      "17.816091954022987 % done,  924.4286684989929  Seconds elapsed\n",
      "Epoch 1, average minibatch 949 loss: 0.197\n",
      "18.80697582243361 % done,  973.6075484752655  Seconds elapsed\n",
      "Epoch 1, average minibatch 999 loss: 0.201\n",
      "19.797859690844234 % done,  1023.3045136928558  Seconds elapsed\n",
      "Epoch 1, average minibatch 1049 loss: 0.185\n",
      "20.788743559254854 % done,  1073.0571711063385  Seconds elapsed\n",
      "Epoch 1, average minibatch 1099 loss: 0.192\n",
      "21.779627427665478 % done,  1123.3172314167023  Seconds elapsed\n",
      "Epoch 1, average minibatch 1149 loss: 0.198\n",
      "22.7705112960761 % done,  1173.4933969974518  Seconds elapsed\n",
      "Epoch 1, average minibatch 1199 loss: 0.207\n",
      "23.76139516448672 % done,  1222.4356548786163  Seconds elapsed\n",
      "Epoch 1, average minibatch 1249 loss: 0.193\n",
      "24.752279032897345 % done,  1272.0153222084045  Seconds elapsed\n",
      "Epoch 1, average minibatch 1299 loss: 0.195\n",
      "25.743162901307965 % done,  1321.199464082718  Seconds elapsed\n",
      "Epoch 1, average minibatch 1349 loss: 0.194\n",
      "26.73404676971859 % done,  1370.6883342266083  Seconds elapsed\n",
      "Epoch 1, average minibatch 1399 loss: 0.184\n",
      "27.724930638129212 % done,  1420.7585635185242  Seconds elapsed\n",
      "Epoch 1, average minibatch 1449 loss: 0.187\n",
      "28.715814506539832 % done,  1470.6674437522888  Seconds elapsed\n",
      "Epoch 1, average minibatch 1499 loss: 0.183\n",
      "29.706698374950456 % done,  1521.003629207611  Seconds elapsed\n",
      "Epoch 1, average minibatch 1549 loss: 0.202\n",
      "30.69758224336108 % done,  1571.7448859214783  Seconds elapsed\n",
      "Epoch 1, average minibatch 1599 loss: 0.181\n",
      "31.6884661117717 % done,  1621.979062795639  Seconds elapsed\n",
      "Epoch 1, average minibatch 1649 loss: 0.190\n",
      "32.67934998018232 % done,  1671.8978865146637  Seconds elapsed\n",
      "Epoch 1, average minibatch 1699 loss: 0.191\n",
      "33.67023384859294 % done,  1722.0563805103302  Seconds elapsed\n",
      "Epoch 1, average minibatch 1749 loss: 0.195\n",
      "34.66111771700357 % done,  1772.2842500209808  Seconds elapsed\n",
      "Epoch 1, average minibatch 1799 loss: 0.191\n",
      "35.65200158541419 % done,  1822.4596750736237  Seconds elapsed\n",
      "Epoch 1, average minibatch 1849 loss: 0.183\n",
      "36.64288545382481 % done,  1872.4000103473663  Seconds elapsed\n",
      "Epoch 1, average minibatch 1899 loss: 0.194\n",
      "37.63376932223544 % done,  1922.1371421813965  Seconds elapsed\n",
      "Epoch 1, average minibatch 1949 loss: 0.194\n",
      "38.62465319064606 % done,  1972.0952084064484  Seconds elapsed\n",
      "Epoch 1, average minibatch 1999 loss: 0.184\n",
      "39.61553705905668 % done,  2021.4339745044708  Seconds elapsed\n",
      "Epoch 1, average minibatch 2049 loss: 0.189\n",
      "40.6064209274673 % done,  2070.9083003997803  Seconds elapsed\n",
      "Epoch 1, average minibatch 2099 loss: 0.202\n",
      "41.597304795877925 % done,  2120.469573497772  Seconds elapsed\n",
      "Epoch 1, average minibatch 2149 loss: 0.186\n",
      "42.588188664288545 % done,  2170.294959306717  Seconds elapsed\n",
      "Epoch 1, average minibatch 2199 loss: 0.189\n",
      "43.579072532699165 % done,  2220.9850277900696  Seconds elapsed\n",
      "Epoch 1, average minibatch 2249 loss: 0.196\n",
      "44.56995640110979 % done,  2271.4840137958527  Seconds elapsed\n",
      "Epoch 1, average minibatch 2299 loss: 0.187\n",
      "45.56084026952041 % done,  2321.517153739929  Seconds elapsed\n",
      "Epoch 1, average minibatch 2349 loss: 0.186\n",
      "46.55172413793103 % done,  2373.503207206726  Seconds elapsed\n",
      "Epoch 1, average minibatch 2399 loss: 0.173\n",
      "47.54260800634166 % done,  2423.656767845154  Seconds elapsed\n",
      "Epoch 1, average minibatch 2449 loss: 0.186\n",
      "48.53349187475228 % done,  2473.926725625992  Seconds elapsed\n",
      "Epoch 1, average minibatch 2499 loss: 0.191\n",
      "49.5243757431629 % done,  2524.143147468567  Seconds elapsed\n",
      "Epoch 1, average minibatch 2549 loss: 0.187\n",
      "50.515259611573526 % done,  2574.4326310157776  Seconds elapsed\n",
      "Epoch 1, average minibatch 2599 loss: 0.183\n",
      "51.506143479984146 % done,  2625.0663821697235  Seconds elapsed\n",
      "Epoch 1, average minibatch 2649 loss: 0.187\n",
      "52.49702734839477 % done,  2675.571860551834  Seconds elapsed\n",
      "Epoch 1, average minibatch 2699 loss: 0.186\n",
      "53.487911216805394 % done,  2726.5708482265472  Seconds elapsed\n",
      "Epoch 1, average minibatch 2749 loss: 0.185\n",
      "54.478795085216014 % done,  2777.149147748947  Seconds elapsed\n",
      "Epoch 1, average minibatch 2799 loss: 0.191\n",
      "55.469678953626634 % done,  2827.7721161842346  Seconds elapsed\n",
      "Epoch 1, average minibatch 2849 loss: 0.178\n",
      "56.460562822037254 % done,  2878.1359288692474  Seconds elapsed\n",
      "Epoch 1, average minibatch 2899 loss: 0.185\n",
      "57.45144669044788 % done,  2928.6576285362244  Seconds elapsed\n",
      "Epoch 1, average minibatch 2949 loss: 0.181\n",
      "58.4423305588585 % done,  2979.346016407013  Seconds elapsed\n",
      "Epoch 1, average minibatch 2999 loss: 0.188\n",
      "59.43321442726912 % done,  3030.092940568924  Seconds elapsed\n",
      "Epoch 1, average minibatch 3049 loss: 0.184\n",
      "60.42409829567975 % done,  3080.2132289409637  Seconds elapsed\n",
      "Epoch 1, average minibatch 3099 loss: 0.185\n",
      "61.41498216409037 % done,  3129.93905210495  Seconds elapsed\n",
      "Epoch 1, average minibatch 3149 loss: 0.188\n",
      "62.40586603250099 % done,  3180.093814134598  Seconds elapsed\n",
      "Epoch 1, average minibatch 3199 loss: 0.181\n",
      "63.396749900911615 % done,  3230.228147983551  Seconds elapsed\n",
      "Epoch 1, average minibatch 3249 loss: 0.182\n",
      "64.38763376932224 % done,  3279.841780900955  Seconds elapsed\n",
      "Epoch 1, average minibatch 3299 loss: 0.179\n",
      "65.37851763773286 % done,  3329.583471059799  Seconds elapsed\n",
      "Epoch 1, average minibatch 3349 loss: 0.181\n",
      "66.36940150614348 % done,  3378.6786546707153  Seconds elapsed\n",
      "Epoch 1, average minibatch 3399 loss: 0.177\n",
      "67.3602853745541 % done,  3428.8947808742523  Seconds elapsed\n",
      "Epoch 1, average minibatch 3449 loss: 0.175\n",
      "68.35116924296473 % done,  3478.311139822006  Seconds elapsed\n",
      "Epoch 1, average minibatch 3499 loss: 0.175\n",
      "69.34205311137535 % done,  3527.238951444626  Seconds elapsed\n",
      "Epoch 1, average minibatch 3549 loss: 0.178\n",
      "70.33293697978597 % done,  3577.115114212036  Seconds elapsed\n",
      "Epoch 1, average minibatch 3599 loss: 0.197\n",
      "71.32382084819659 % done,  3626.633081436157  Seconds elapsed\n",
      "Epoch 1, average minibatch 3649 loss: 0.187\n",
      "72.31470471660721 % done,  3676.5671718120575  Seconds elapsed\n",
      "Epoch 1, average minibatch 3699 loss: 0.174\n",
      "73.30558858501783 % done,  3725.7595360279083  Seconds elapsed\n",
      "Epoch 1, average minibatch 3749 loss: 0.174\n",
      "74.29647245342846 % done,  3775.64288520813  Seconds elapsed\n",
      "Epoch 1, average minibatch 3799 loss: 0.169\n",
      "75.28735632183908 % done,  3825.4652631282806  Seconds elapsed\n",
      "Epoch 1, average minibatch 3849 loss: 0.168\n",
      "76.2782401902497 % done,  3875.60436296463  Seconds elapsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average minibatch 3899 loss: 0.169\n",
      "77.26912405866032 % done,  3925.3027029037476  Seconds elapsed\n",
      "Epoch 1, average minibatch 3949 loss: 0.193\n",
      "78.26000792707094 % done,  3975.8148572444916  Seconds elapsed\n",
      "Epoch 1, average minibatch 3999 loss: 0.187\n",
      "79.25089179548156 % done,  4025.348587036133  Seconds elapsed\n",
      "Epoch 1, average minibatch 4049 loss: 0.169\n",
      "80.2417756638922 % done,  4075.4416732788086  Seconds elapsed\n",
      "Epoch 1, average minibatch 4099 loss: 0.170\n",
      "81.23265953230282 % done,  4125.597173690796  Seconds elapsed\n",
      "Epoch 1, average minibatch 4149 loss: 0.172\n",
      "82.22354340071344 % done,  4175.533425092697  Seconds elapsed\n",
      "Epoch 1, average minibatch 4199 loss: 0.179\n",
      "83.21442726912406 % done,  4225.82568693161  Seconds elapsed\n",
      "Epoch 1, average minibatch 4249 loss: 0.173\n",
      "84.20531113753468 % done,  4275.221361398697  Seconds elapsed\n",
      "Epoch 1, average minibatch 4299 loss: 0.172\n",
      "85.1961950059453 % done,  4325.191174030304  Seconds elapsed\n",
      "Epoch 1, average minibatch 4349 loss: 0.197\n",
      "86.18707887435592 % done,  4375.041222572327  Seconds elapsed\n",
      "Epoch 1, average minibatch 4399 loss: 0.193\n",
      "87.17796274276655 % done,  4425.0539536476135  Seconds elapsed\n",
      "Epoch 1, average minibatch 4449 loss: 0.179\n",
      "88.16884661117717 % done,  4475.684318065643  Seconds elapsed\n",
      "Epoch 1, average minibatch 4499 loss: 0.183\n",
      "89.1597304795878 % done,  4526.748071193695  Seconds elapsed\n",
      "Epoch 1, average minibatch 4549 loss: 0.174\n",
      "90.15061434799841 % done,  4576.937345266342  Seconds elapsed\n",
      "Epoch 1, average minibatch 4599 loss: 0.177\n",
      "91.14149821640903 % done,  4629.031295776367  Seconds elapsed\n",
      "Epoch 1, average minibatch 4649 loss: 0.177\n",
      "92.13238208481965 % done,  4680.741712093353  Seconds elapsed\n",
      "Epoch 1, average minibatch 4699 loss: 0.179\n",
      "93.12326595323029 % done,  4732.352080106735  Seconds elapsed\n",
      "Epoch 1, average minibatch 4749 loss: 0.192\n",
      "94.11414982164091 % done,  4783.616394281387  Seconds elapsed\n",
      "Epoch 1, average minibatch 4799 loss: 0.175\n",
      "95.10503369005153 % done,  4835.201779842377  Seconds elapsed\n",
      "Epoch 1, average minibatch 4849 loss: 0.178\n",
      "96.09591755846215 % done,  4886.468389749527  Seconds elapsed\n",
      "Epoch 1, average minibatch 4899 loss: 0.190\n",
      "97.08680142687277 % done,  4937.464879751205  Seconds elapsed\n",
      "Epoch 1, average minibatch 4949 loss: 0.192\n",
      "98.07768529528339 % done,  4988.3084597587585  Seconds elapsed\n",
      "Epoch 1, average minibatch 4999 loss: 0.174\n",
      "99.06856916369402 % done,  5040.265860557556  Seconds elapsed\n",
      "Finished 1 epochs of training\n",
      "Training complete after 0 epochs\n"
     ]
    }
   ],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "start = time.time()\n",
    "\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "    N = 50\n",
    "    N_minibatch_loss = 0.0    \n",
    "\n",
    "    min_loss = 100.0\n",
    "    early_stop_count = 0 \n",
    "   \n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "\n",
    "        if (minibatch_count + 1) % N == 0:    \n",
    "\n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "            print(100 * minibatch_count/len(train_loader),\"% done, \" ,time.time() - start, \" Seconds elapsed\")\n",
    "\n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "    \n",
    "    # Implement cross-validation for each epoch\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_count, (images, labels) in enumerate(val_loader, 0):\n",
    "            images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "\n",
    "    if val_loss >= min_loss:\n",
    "        early_stop_count += 1\n",
    "\n",
    "        if early_stop_count == 5:\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        early_stop_count = 0\n",
    "        min_loss = val_loss\n",
    "   \n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "print(\"Training complete after\", epoch, \"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Aggregated Scores -----\n",
      "Minibatch 49 accuracy: 0.9496428571428572\n",
      "Minibatch 49 precision: 0.04\n",
      "Minibatch 49 recall: 0.004657509157509157\n",
      "Minibatch 49 bcr: 0.022328754578754578\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 99 accuracy: 0.9474107142857143\n",
      "Minibatch 99 precision: 0.05\n",
      "Minibatch 99 recall: 0.0079819154275676\n",
      "Minibatch 99 bcr: 0.028990957713783797\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 149 accuracy: 0.9544642857142859\n",
      "Minibatch 149 precision: 0.02333333333333333\n",
      "Minibatch 149 recall: 0.005714285714285714\n",
      "Minibatch 149 bcr: 0.014523809523809524\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 199 accuracy: 0.9492857142857143\n",
      "Minibatch 199 precision: 0.02\n",
      "Minibatch 199 recall: 0.00419047619047619\n",
      "Minibatch 199 bcr: 0.012095238095238095\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 249 accuracy: 0.9503571428571428\n",
      "Minibatch 249 precision: 0.01\n",
      "Minibatch 249 recall: 0.00125\n",
      "Minibatch 249 bcr: 0.005625\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 299 accuracy: 0.9484821428571429\n",
      "Minibatch 299 precision: 0.02\n",
      "Minibatch 299 recall: 0.0023333333333333335\n",
      "Minibatch 299 bcr: 0.011166666666666667\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 349 accuracy: 0.9447321428571429\n",
      "Minibatch 349 precision: 0.08333333333333331\n",
      "Minibatch 349 recall: 0.012643724696356275\n",
      "Minibatch 349 bcr: 0.0479885290148448\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 399 accuracy: 0.9488392857142859\n",
      "Minibatch 399 precision: 0.03166666666666666\n",
      "Minibatch 399 recall: 0.005841093117408907\n",
      "Minibatch 399 bcr: 0.018753879892037785\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 449 accuracy: 0.9513392857142858\n",
      "Minibatch 449 precision: 0.08333333333333331\n",
      "Minibatch 449 recall: 0.012663419913419915\n",
      "Minibatch 449 bcr: 0.047998376623376623\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 499 accuracy: 0.9516964285714283\n",
      "Minibatch 499 precision: 0.02\n",
      "Minibatch 499 recall: 0.00404040404040404\n",
      "Minibatch 499 bcr: 0.01202020202020202\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 549 accuracy: 0.9479464285714286\n",
      "Minibatch 549 precision: 0.056666666666666664\n",
      "Minibatch 549 recall: 0.008518461930226635\n",
      "Minibatch 549 bcr: 0.032592564298446654\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 599 accuracy: 0.9449999999999998\n",
      "Minibatch 599 precision: 0.04666666666666666\n",
      "Minibatch 599 recall: 0.007086439846030639\n",
      "Minibatch 599 bcr: 0.02687655325634865\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 649 accuracy: 0.9491964285714286\n",
      "Minibatch 649 precision: 0.05\n",
      "Minibatch 649 recall: 0.008023310023310025\n",
      "Minibatch 649 bcr: 0.029011655011655008\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 699 accuracy: 0.9499107142857143\n",
      "Minibatch 699 precision: 0.02\n",
      "Minibatch 699 recall: 0.00525\n",
      "Minibatch 699 bcr: 0.012624999999999999\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 749 accuracy: 0.9456250000000002\n",
      "Minibatch 749 precision: 0.03666666666666667\n",
      "Minibatch 749 recall: 0.007441520467836257\n",
      "Minibatch 749 bcr: 0.02205409356725146\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 799 accuracy: 0.9454464285714286\n",
      "Minibatch 799 precision: 0.04\n",
      "Minibatch 799 recall: 0.005922196796338672\n",
      "Minibatch 799 bcr: 0.022961098398169337\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 849 accuracy: 0.9465178571428572\n",
      "Minibatch 849 precision: 0.04\n",
      "Minibatch 849 recall: 0.005381598793363499\n",
      "Minibatch 849 bcr: 0.022690799396681748\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 899 accuracy: 0.945625\n",
      "Minibatch 899 precision: 0.04666666666666666\n",
      "Minibatch 899 recall: 0.007301587301587301\n",
      "Minibatch 899 bcr: 0.026984126984126985\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 949 accuracy: 0.9500000000000002\n",
      "Minibatch 949 precision: 0.03\n",
      "Minibatch 949 recall: 0.004455128205128205\n",
      "Minibatch 949 bcr: 0.0172275641025641\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 999 accuracy: 0.9483035714285716\n",
      "Minibatch 999 precision: 0.03\n",
      "Minibatch 999 recall: 0.004606643356643357\n",
      "Minibatch 999 bcr: 0.017303321678321677\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 1049 accuracy: 0.9476785714285713\n",
      "Minibatch 1049 precision: 0.05\n",
      "Minibatch 1049 recall: 0.007231924508240297\n",
      "Minibatch 1049 bcr: 0.02861596225412015\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 1099 accuracy: 0.9449107142857143\n",
      "Minibatch 1099 precision: 0.01\n",
      "Minibatch 1099 recall: 0.0015384615384615385\n",
      "Minibatch 1099 bcr: 0.005769230769230769\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 1149 accuracy: 0.9486607142857144\n",
      "Minibatch 1149 precision: 0.04\n",
      "Minibatch 1149 recall: 0.008508658008658009\n",
      "Minibatch 1149 bcr: 0.024254329004329004\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 1199 accuracy: 0.9478571428571427\n",
      "Minibatch 1199 precision: 0.04\n",
      "Minibatch 1199 recall: 0.00487012987012987\n",
      "Minibatch 1199 bcr: 0.022435064935064933\n",
      "----- Aggregated Scores -----\n",
      "Minibatch 1249 accuracy: 0.9516964285714284\n",
      "Minibatch 1249 precision: 0.04\n",
      "Minibatch 1249 recall: 0.006925647451963241\n",
      "Minibatch 1249 bcr: 0.02346282372598162\n",
      "[[0.         0.         0.5        0.5        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.0862069  0.03448276 0.43965518 0.19827586 0.04310345 0.01293103\n",
      "  0.01724138 0.01724138 0.06465517 0.04310345 0.00862069 0.\n",
      "  0.02586207 0.00862069]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "agg_accuracies = []\n",
    "agg_precisions = []\n",
    "agg_recalls = []\n",
    "agg_BCRs = []\n",
    "\n",
    "class_accuracies = []\n",
    "class_precisions = []\n",
    "class_recalls = []\n",
    "class_BCRs = []\n",
    "\n",
    "batch_start = 0\n",
    "N = 50\n",
    "\n",
    "conf_mtx = np.zeros((14, 14), dtype = np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the next minibatch of images, labels \n",
    "    for minibatch_count, (images, labels) in enumerate(test_loader, 0):\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        predicted = torch.round(outputs.data)\n",
    "        \n",
    "        # Convert from Cuda tensor -> numpy array\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        conf_mtx = confusion_matrix(conf_mtx, predicted, labels)\n",
    "        \n",
    "        # Compute aggregated scores\n",
    "        acc = accuracy(predicted, labels)\n",
    "        pre = precision(predicted, labels)\n",
    "        rec = recall(predicted, labels)\n",
    "        bcr = BCR(predicted, labels)\n",
    "        \n",
    "        agg_accuracies.append(acc)\n",
    "        agg_precisions.append(pre)\n",
    "        agg_recalls.append(rec)\n",
    "        agg_BCRs.append(bcr)\n",
    "        \n",
    "        # Compute scores by class\n",
    "        acc = accuracy_per_class(predicted, labels)\n",
    "        pre = precision_per_class(predicted, labels)\n",
    "        rec = recall_per_class(predicted, labels)\n",
    "        bcr = BCR_per_class(predicted, labels)\n",
    "       \n",
    "        class_accuracies.append(acc)\n",
    "        class_precisions.append(pre)\n",
    "        class_recalls.append(rec)\n",
    "        class_BCRs.append(bcr)\n",
    "                \n",
    "        if (minibatch_count + 1) % N == 0: \n",
    "            # Print the loss averaged over the last N mini-batches\n",
    "            print('----- Aggregated Scores -----')\n",
    "            print_scores(batch_start, minibatch_count, agg_accuracies, \n",
    "                         agg_precisions, agg_recalls, agg_BCRs, aggregate=True)\n",
    "            '''\n",
    "            print('----- Scores By Class -----')\n",
    "            print_scores(batch_start, minibatch_count, class_accuracies, \n",
    "                         class_precisions, class_recalls, class_BCRs, aggregate=False)\n",
    "            '''\n",
    "            batch_start = minibatch_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each row of confusion matrix\n",
    "for j in range(conf_mtx.shape[0]):\n",
    "    if np.sum(conf_mtx[j]) != 0:\n",
    "        conf_mtx[j] /= np.sum(conf_mtx[j]) \n",
    "        \n",
    "print(conf_mtx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
